CONTEXTUALDOCUMENTEMBEDDINGSJohnX.MorrisCornellUniversityjxm3@cornell.eduAlexanderM.RushCornellUniversityarush@cornell.eduABSTRACTDensedocumentembeddingsarecentraltoneuralretrieval.Thedominantparadigmistotrainandconstructembeddingsbyrunningencodersdirectlyonindividualdocuments.Inthiswork,wearguethattheseembeddings,whileeffective,areimplicitlyout-of-contextfortargetedusecasesofretrieval,andthatadocumentembeddingshouldtakeintoaccountboththedocumentandneighboringdocumentsincontext–analogoustocontextualizedwordembeddings.Weproposetwocom-plementarymethodsforcontextualizeddocumentembeddings:rst,analternativecontrastivelearningobjectivethatexplicitlyincorporatesdocumentneighborsintotheintra-batchcontextualloss;second,anewcontextualarchitecturethatexplicitlyencodesneighbordocumentinformationintotheencodedrepresentation.Resultsshowthatbothmethodsachievebetterperformancethanbiencodersinseveralsettings,withdifferencesespeciallypronouncedout-of-domain.Weachievestate-of-the-artresultsontheMTEBbenchmarkwithnohardnegativemining,scoredistillation,dataset-specicinstructions,intra-GPUexample-sharing,orextremelylargebatchsizes.Ourmethodcanbeappliedtoimproveperformanceonanycontrastivelearningdatasetandanybiencoder.1INTRODUCTIONMachinelearningapproachestotextretrievalaimtolearnanembeddedrepresentationforindexingdocuments.Classically,thisareawasdominatedbystatisticalapproachesusingsparselexicalmatchingmethodsbasedonn-gramfrequenciessuchasBM25(Robertson&Zaragoza,2009).Onlyrecentlyhaveneuralnetworksbecomecompetitivewithstate-of-the-artmodelsonretrievaltasks(Karpukhinetal.,2020;Thakuretal.,2021).Theprimaryneuralmethodisadualencoderarchitecturethatindependentlyencodesbothadocumentandquerytoadenselatentspaceforretrievallookup.Thisdocumentembeddingspacecanimproveuponastatisticalmodelsinceitislearnedend-to-endforretrieval.However,thereisatleastonenotablebenetofstatisticalapproachesthatislostbyneuralmodels.Statisticalmodelscaneasilyincorporatepriorcorpusstatisticssuchasinversedocumentfrequency(IDF),intotheirrepresentation.Thispriortermimpartscontext-dependenceontothemodel,sinceitcanbeupdatedbasedoninformationspecictoretrievalinagivendomainattesttime.Wecontrastthiscontextualformulationwithneuraldocumentencodersthatarebydenitionafunctionofthedocumentitself.Forexampleconsiderthefollowingdocument:TheNationalFootballLeagueDraftisanannualeventinwhichtheNationalFootballLeague(NFL)teamsselecteligiblecollegefootballplayers...Dependingontheretrievaldomain,e.g.Wikipediasearch,sportsarticles,ortelevisedevents,IDFmayweighttermssuchasNFL,draftorannualhigher;aneuraldocumentembeddingmodelwouldneedtoselectaglobalweightingforthisdocument.Inthiswork,weexplorecontextualizationofdocumentembeddingsproducedbydenseencoders.Thegoalistoproduceembeddingsthatarebetterabletohandleretrievaltasksinspecicchallengingcontexts.Weproposetwocomplementarychangestodocumentencoders:acontextualtrainingprocedureandarchitecture.Forcontextualtraining,weaimtobuildanotionofneighboringdocumentsdirectlyintothecontrastivelearningprocess.Weproposeamethodthatusesonfastquery-documentclusteringtoproducea1arXiv:2410.02525v2 [cs.CL] 7 Oct2024

Figure1:Overviewofoursystemforcontextualdocumentembeddings(CDE).Ourmodeloperatesintwostages:arststageusedtocharacterizethedatasetfromsamples,andasecondstageusedtoembedthenaldocument.groupofneighborsforeachtrainingbatch.Eachupdatefortrainingisconstructedpurelyfromneighboringdocumentstoensurethatembeddingscandistinguishdocumentseveninthemostchallengingcontexts.Forthearchitecture,weproposeanewencoderthatinjectsinformationaboutthecontextualdocu-mentsduringembedding.TheproposedarchitectureaugmentsthestandardBERT-styleencoderwithadditionalconditioningthatprovidesaggregateddocument-levelinformationaboutneighboringdoc-uments.WecallourmethodC

ocumentE

ontextualD

mbedding(CDE).Analogouslytopre-computedcorpus-levelstatistics,thismethodprovidesamannerfortheembeddingtotakeintoaccounttherelativefrequencyoftermsincontext.Thenaloutputisstillanembeddingofthesamesize,sothisdoesnotrequireanyadditionalstorageorotherchangestotheretrievalprocess.Whenindexing,weutilizeinformationfromthecorpustoproducedocumentandqueryembeddingsthatarespecictoaparticulardomain.Experimentscomparethesetwoextensionstostandardapproachesfortrainingdocumentembeddings.Ourresultsshowthatcontextualcontrastivetrainingimprovesstandardtextembeddingmodeltraining,andcanberunwithoutotherapproachessuchasadditionalhardnegatives.Withthecontextualencoderarchitecture,weseeadditionalimprovementsoverabaselinemodelinallsettingstested,withlargerimprovementsinhighlyspecicdomainssuchassmalldatasetsofnancialandmedicaldocuments.Whentrainedatindustry-scale,ourmodelachievesstate-of-the-artresultsforsmall(<250Mparameter)modelsontheMTEBbenchmark.2RELATEDWORKTextretrieval.Ourworkisrelatedtothegeneraleldoftextretrieval;weproposespecicimprovementstothetrainingof“biencoder”textembeddingmodelssuchasDPR(Karpukhinetal.,2020),GTR(Nietal.,2021),Contriever(Izacardetal.,2022),LaPraDoR(Xuetal.,2022),Instructor(Suetal.,2023),Nomic-Embed(Nussbaumetal.,2024),E5(Wangetal.,2024),andGTE(Lietal.,2023).Wefocusontheproblemofadaptingthesetextretrievalmodelstonewcorporaattesttime;somepriorworkhasnotedthisproblem(Daietal.,2022;Sciavolino,2021)andproposedsolutionssuchasunsupervisedspan-samplingandtrainingontestcorpora(Gao&Callan,2021)anddistillationonthetestcorpusfromareranker(Sungetal.,2023).Lateinteractionmethods(Khattab&Zaharia,2020;Santhanametal.,2022)alsoofferonewaytoimproveout-of-domainretrievalperformance,butincreasetheruntimeandcomplexityofsearch.Weproposeabettersamplingschemethatcanbeusedtotrainanybiencoderorlateinteractionmodelaswellasatraining-freemethodfortest-timeadaptation.2

Contrastivelearning.MuchresearchhasfocusedontheeffectofhardnegativesontheperformanceofcontrastivelearningmethodsChenetal.(2020);Quetal.(2021);Robinsonetal.(2021);Wangetal.(2023).(Zhang&Stratos,2021)observethathardernegativesprovideabetterapproximationoftheoverallcross-entropyloss,butdonotconsiderbatch-leveloptimizationsfornegativeselection.Hofst¨atteretal.(2021)clusterqueriesbeforetrainingandshowthatthisimprovesperformance.Sachidanandaetal.(2023)alsoconsidercontrastivebatchsamplingasaglobaloptimizationproblem,butdonotapplytheirtechniquetostate-of-the-arttransformer-basedtextembeddingmodels.(Maetal.,2024)useaclusteringalgorithmtopartitionadatasetintoseveralsub-datasets,buttrainadifferentmodeloneachsub-dataset.Ourtrainingalgorithmaimstondthehardestpossiblebatchestotraintextembeddingmodel.Test-timeadaptation.Ourmethodcanbecomparedtoothersolutionstotest-timeadaptation,aproblemthathasbeenwell-studiedacrossavarietyofdomains(Jangetal.,2023).Inretrieval,oneformoftest-timeadaptationispseudo-relevancefeedback(PRF)(Rocchio,1971;Lietal.,2018;Wangetal.,2021),wheredocumentsrelevanttothequeryareusedtoconstructanal,enhancedqueryrepresentation.Thequerysideofourmodelcanbeseenasaformofpseudo-relevancefeedback;however,wetrainfromscratchtosupportamoregeneralformofPRFnatively,onthedocumentrepresentationaswellasthequery.Non-parametricmodeling.Ourcontextualdocumentmodelcanbeseenasaformofnon-parametricmodeling.Thisshowsconnectionswiththealargebodyofdeeplearningresearchsuchasthenon-parametrictransformer(NPT)(Kossenetal.,2022)andthesubeldofNeuralProcesses(Garneloetal.,2018;Kimetal.,2019;Nguyen&Grover,2023).Semi-parametricmodelshavebeenrecentlyappliedinNLP,specicallytothetaskoflanguagemodeling(Borgeaudetal.,2022;Khandelwaletal.,2020).Insteadofusingaretrievalmodeltobuildasemi-parametriclangaugemodel,webuildasemi-parametricmodelspecicallyforthetaskofretrieval.3BACKGROUNDWecanviewtextretrievalmethodsprobabilisticallyascomputingadistributionoverpotentialdocumentsbasedonascalarscorefunctionf(d,q)matchingdocumentsandqueries:p(dq)=expf(d,q)

d′∈Dexpf(d′,q)(1)whereDisanitesetofdocumentsinadataset.Thereisawidevarietyofdifferentdenitionsforfincludingfullpairwiseneuralparameterizations(Nogueira&Cho,2020).Inthiswork,wefocusonefcientretrievalmethodsusingvector-basedmethods,alsoknownasembeddingmodels.Vectorretrievalmethodsassumethatf(d,q)canbefactoredintotwoembeddingterms,ϕ(d)·ψ(q),thedocumentandqueryembeddingrespectively.Thisfactorizationallowsprecomputationofthedocumentembeddingsϕ(d)foralld∈D.Thisiscriticalforfacilitatingfastcomputationofargmaxdp(dq)ortop-kvariants(Douzeetal.,2024).Instatisticalretrieval,ϕandψareclosed-formfunctionsofthedata,oftenrepresentingunigramorbigramcountsbytherelativefrequencyofwordtypes.Notablyforthiswork,thesemethodscanalsoutilizedistributionalpropertiesofthetestdatasetasaprior,forexamplethroughinversedocumentfrequency(IDF).Werepresentthisintegrationofdataset-levelinformationbywritingthevectorproductϕ(d;D)·ψ(q;D).Inneuralretrieval,weinsteadlearntherepresentationasadensevector.Weassumeaccesstoatrainingcorpusofdocumentandquerypairs(thesemaybesupervised,i.e.gold-standardannotations,orunsupervised,i.e.noisedsyntheticexamples),DT=(d1,q1),...,(dJ,qJ),withtheaimoflearningtheembeddingfunctionϕandψ.Trainingcanbemotivatedasmaximizinglikelihoodofthedocumentcorrespondingtoeachquery,i.e.jlogp(djqj).Unfortunately,sinceretrievaldatasetscanhaveDexceedmillionsofdocuments,computingthenormalizerinEq1ateachtrainingstepisnotanoption.Insteadcontrastivelearningisusedwherethelikelihoodisreplacedwithabiasedapproximationcalculatedfromnegativesamples:3

maxϕ,ψjlogp(djqj)≈jlogexpf(dj,qj)

d′∈H(qj)expf(d′,qj)whereHisasetofexamplesusedtoapproximatethenormalizingconstant.Inimplementation,inadditiontothesehardnegativeexamples,otherexamplesfromthemini-batcharealsousedtocomputethenormalizersinceitrequiresnoadditionalcomputeforcalculatingϕ(d).4METHODSInourwork,weareinterestedinintegratingcontextualinformationintoourembeddingfunctionsϕandψ.Thestandardneuralϕispurelyafunctionofthedocumentϕ(d)anddoesnottakeintoaccountanynotionofcontext.Thiscontrastswiththestatisticalmodelϕ(·;D)andψ(·;D).Arguablythisisnotanissueifretrievaliscompletelyindomain,asϕiscapableoflearningstatisticssuchasIDFandaveragedocumentlengthonthetrainingsetthroughgradientdescent.However,inmanyretrievalbenchmarks,modelsaretrainedoverasinglesetofdocumentsDandthentestedinmanyotherdomainsDthatdifferssignicantlyfromDT.Inthissetting,trainingonDTalonemaynotbeabletoproviderobustembeddingswhenusedincontextssuchasD.4.1CONTEXTUALTRAININGWITHADVERSARIALCONTRASTIVELEARNINGReturningtotheexamplefromtheintroduction,weassumethatinageneralpurposetrainingcorpusDT,thetermNFLisararewordappearinginrelativelyfewdocumentsandausefulsignal.However,ifattesttimeDisacorpusofsportsarticles,thiswordwouldbeexceedinglycommon.Evaluationinthisdomainis,inastatisticalsense,adversarialtotheoriginaldataset.Tohandlethisissue,meta-learning-styleobjectiveshaveshowntobeeffectivefortrainingdocumentembedders.Intheseapproaches,insteadofsamplingdocuments-querypairsiid,theobjectiverstsampleadomainandthensampleabatchofexamples.Thisensuresthatthemodelmostlyseesrelatedtrainingpointsineachdomain.Weproposeatrainingobjectivethatsynthesizesalargesetofne-graineddomainstotrainthemodelon.Formally,ouraimistopartitionthetrainingdatasetDTintogroups(B1,...BB)suchthateachgrouprepresentsaself-similarpseudo-domain:maxϕ,ψb(d,q)∈Bblogp(dq)=maxϕ,ψb(d,q)∈Bblogexpf(d,q)

(d′,·)∈Bbexpf(d′,q)Computationally,theinnertermcanbeimplementedasasinglebatchandcomputedefcientlywithouttheneedforseparatehardnegatives(H).Ideallywewantgroupsthatareaschallengingaspossible.Zhang&Stratos(2021)showthatincreasingthepartitiontermimprovesthecontrastiveapproximationtothemaximumlikelihoodthegradient.Wecanformalizethesearchforthemostdifcultcongurationofbatchesasanoptimizationproblem:max(B1,BB)b(d,q)∈Bb(d′,q′)∈Bbf(d,q′)+f(d′,q)=max(B1,BB)b(d,q)∈Bb(d′,q′)∈Bbϕ(d)·ψ(q′)+ϕ(d′)·ψ(q)(2)Solvingthiscombinatorialobjectiveexactlyisintractable,butwecantreatapproximateasolutionusingclustering.WerstmovefromamaximizationtoaminimizationbyreplacingthetwodotproductswithL2,i.e.m((d,q),(d′,q′))=ϕ(d)−ψ(q′)+ϕ(′d)−ψ(q)whichisequivalentfornormalizedembeddings.Wethennotethattreatedassymmetricpairs,thistermobeysthetriangleinequalityforanyotherpairmi.e:m((d,q),m)+m(m,(d′,q′))≥m((d,q),(d′,q′))4

expf(d,q)+d′∈S(q,d)expf(d′,q)(4)Forsimplicity,weagainselectftobeasimplepre-trainedembeddingmodel.Thismethodlikelyover-prunessomepotentialtruenegativesfoundbythesurrogatemodel;howeverwefoundittobecriticaltomodelaccuracy.Packing.Clustersfoundbyouralgorithmwillbeofvaryingsizes,andneedtobepackedintoequal-sizedbatches.Weapplyapost-hocprocedure.Weconsiderbothrandompartitioningandgroupingviagreedycluster-leveltravelingsalesman,similartoShietal.(2024).Inbothcases,wesplitlargegroupintointosmallerbatches,andmergeclosesmallbatchesfromwithinthesamedomainintoevenly-sizedbatches.Thishasanaddedbenetofintroducingrandomnessintothegroupswhentrainingformultipleepochs.WeleaveittofutureworktoanalyzethefulleffectsofdifferentpackingstrategiessuchasexpensiveBalancedK-MeansorheuristicapproachessuchasEqualK-Means(Gururanganetal.,2023).4.2CONTEXTUALDOCUMENTEMBEDDING(CDE)Contextualizationcanalsobeaddeddirectlytothearchiecture.Takinginspirationfromsparsevectorretrievalwhichusescorpusstatisticstodeterminetheformoftheembedding,wemodifytheencoderstohaveaccesstothecorpusitself,i.e.ϕ(d;D)andψ(d;D).Thiseffectivelyaugmentsthebiencodermodeltogiveittheabilitytocontextualizedocumentsdirectly.Themainchallengeishowtodesignaneuralarchitecturethatcantakeintoaccountdatasetcontex-tualization.Ononeextreme,wecouldfollowmethodslikeBM25andprecomputeaxedsetofcorpusstatisticsthatcouldbefedtothedocumentencoder.Ontheotherextreme,wecouldallowtheencoderfullaccesstotheentirecorpus,throughsomeformofcrossattention.Thelatterapproachhasbeenexploredonasmallscaleinmethodslikeneuralprocesses(Garneloetal.,2018);however,itwouldbedifculttoscaletolargerdatasets.Weoptforamiddlegroundthatallowsthemodeltolearncorpusstatistics,butisalsorelativelyefcienttocompute,showninFigure1.Specically,wenotethatdocumentembeddingsretainasurprisingamountoflexicalinformationevenafterembedding(Morrisetal.,2023).Therefore,ifwepre-embedasubsetofthecorpus,webelievewecanstilldynamicallycalculatekeydatasetinformationduringencoding.5

Thisimpliesthatthefollowingcentroid-basedobjectiverepresentsanupper-boundonouroriginalobjective:min(B1,BB)(m1,,mB)b(d,q)∈Bbm((d,q),mb)(3)ForaknownsizeB,thisdenesanasymmetricK-Meansclusteringproblem.AsolutioncanbeefcientlycomputedusingextremelyfastEuclideanK-Meanspackagesbetreatingeachdatapointastwoseparatevectorsϕ(d)⊕ψ(q)andψ(q)⊕ϕ(d)where⊕isconcatenation.ClusterEmbeddings.Sinceclusteringisperformedbeforetraining,wedonothavedenseencodersforϕandψwhenconstructingthegroups.Borrowingmethodsfromhard-negativemining(Robinsonetal.,2021)wecanreplacetheϕandψwithasimplerembeddingmodelwhenconstructinggroups.Weexperimentwithasparsevectorrepresentationandwithpretraineddenserepresentations,settlingonGTR(Nietal.,2021),apopularandgenerictextembeddingmodel.FilteringFalseNegatives.Ourmethodisespeciallysensitivetofalsenegatives,astheywillbemorelikelytobeincludedinagivenbatch.Unfortunately,traditionalretrievaldatasetsarenotdesignedwiththistypeofglobalobjectiveinmind:falsenegativesarecommoninmostretrievaldatasetsandtheirprevalenceincreaseswithdatasetscale.Asonedatapoint,Quetal.(2021)foundthatover70%oftop-retrievedpassagesinMSMarcoarefalsenegatives.Toavoidasituationwhereeachbatchcontainsalargenumberoffalsenegatives,wecomputeanequivalenceclass:S(q,d)=d′∈Df(q,d′)≥f(q,d)forsomesurrogatescoringfunctionf.Attrainingtime,wealterthepartitionfunctionfordsothatitnolongerincludestheelementsofS(q,d),whicharenotdenitivelynegativeexamples:logp(dq)=expf(d,q)

Weproducecontextualizedembeddingsviaatwo-stageprocess:Firststage:Gatherandembedcontext.Givencontextdocumentsd1,...,dJ∈D,weembedeachusingauniqueembeddingmodelandconcatenateembeddingsintoasequenceM1(d1)...M1(dJ).Secondstage:Embeddocumentwithadditionalcontexttokens.Tocomputeϕfordocumentd′weintegratecontextualembeddingsequenceattheinputofsecond-stageembeddingmodelM2:ϕ(d′;D)=M2(M1(d1),...,M1(dJ),E(d′1),...,E(d′T))(5)HereM1istherst-stageencodermodel,M2isasecond-stageencodermodel,andEisthetokenembeddingmatrixofM2appliedtoeachtokenind′.Inpractice,weparameterizebothM1andM2usingtraditionalbidirectionaltransformers,soourmodeliscomprisedoftwobiencoder-likebackbonescalledinsequence.Thereisasimilarcontextualizedmodelforthequeryencoderψwhichisalsogivendocumentcontext(aswedonothavequerycontextattesttime):ϕ(q;D)=M2(M1(d1),...,M1(dJ),E(q1),...,E(qT))(6)Wenoteseveralimplementationpropertiesofthisarchitecture.Duringtraining,computingcontextualembeddingsforeachcontextualdocumentforeachtraininginstancewouldnaivelyincreasetrainingbyacomputationalfactorproportionaltoJ,thenumberofdocumentsincontext.Thistimeincreasewouldnotbetractable,sincecontrastivetrainingcanalreadytakemanydays.Weovercomethisdifcultybysharingcontextd1,...,dJwithinabatchofdocuments;thisallowsustocomputerepresentationsjustoncepertrainingstepandreusethembetweendocumentsviacomputationalgraph.1WhenindexinganewcorpusD,rststagerepresentationsM1(d1)...M1(dJ)canbecomputedonceandcached,soM1doesnotaddparametersorruntimetothesearchprocess.Queryrepresentationscanalsousethecachedcontext,whichonlyrequireadditionalinputstotheencoder.(Ourmodeldoesnotincludecontextualizedqueries,onlydocuments,aswetypicallydonotassumeaccesstoexamplequeriesattest-time.)Embeddingwithoutcontext.Individualcorporaduringtrainingmaynothavesufcientoravailablecontext.Toimproveourmodel’sgeneralization,weusesequencedropout,wherewerandomlyreplacecontextembeddingsM1(d∗)withsomenulltokenv∅accordingtosomeauniformprobabilityp.Attesttime,ifnocorpusinformationisavailable,ourmodelcannowfunctionasanon-contextualbiencodersimplybyreplacingallsequencetokeninputswithv∅.Position-agnosticembedding.SincedocumentsofDareunordered,weremoveallpositionalityfromtheneuralencodings.Whenparameterizingθwithatraditionaltransformer,thiscanbeachievedbyomittingpositionalembeddingsatthepositionscorrespondingtoD.Inpractice,weusetransformersimplementationsdependentonFlashAttentionwithrotarypositionalembeddingsateachself-attentionlayer.FulldetailsofhowwedisablepositionalityareavailableinSection10.4.Two-stagegradientcaching.Toimprovetrainingweemployagradient-cachingtechniqueanalo-goustoatwo-stageversionofGradCache(Gaoetal.,2021).Thistechniqueallowsustotlargerbatches,longersequenceswithmorecontextualsampleswithoutrunningoutofmemory.Essentially,wecomputerst-stageandsecond-stagerepresentationsindependentlywithoutgradients.Wethenusethesefrozenrepresentationstocomputetheloss,andgradientswithrespecttothesecond-stagerepresentations.Wethenre-runthesecondstagewithgradientsenabledandusetheoutputgradientstobackpropagatethroughthesecond-stagemodel,andobtaingradientsfortherst-stagerepresenta-tions.Werepeatthisprocessfortherst-stagerepresentations.Thisallowsustotradeoffcomputation(runningeachtransformerforwardpasstwice)formemory.

1Contextreuseisonlyfeasiblebecausedocumentswithinthesamebatchtypicallysharealargeamountofcontextanyway,sincetheyareclustered.6

63.1

ContextualBatchArch

BatchSizeClusterSize

16384-

Table1:Performanceofoursmallmodelswithandwithoutthetwoimprovementsproposedinthispaper,measuredonashortenedversionoftheBEIRbenchmark.NumbersareNDCG@10.5EXPERIMENTALSETUPWeconsiderarangeofretrievalexperimentsacrossdifferentscales.Torunexperimentsacrossasuit-ablenumberofsettings,wedeviseasmallsetting:six-layertransformer,maximumsequencelengthof64,andmaximumnumberof64additionalcontextualtokens.Inthisscenario,weevaluateonatruncatedversionoftheBEIRbenchmark(Thakuretal.,2021).Giventhelowcostofeachexperiment,weareabletopre-trainandne-tunebothbiencoderandcontextualmodelsacrossavarietyofbatchsizesin256,512,1024,2048,4096andclustersizes64,256,1024,4096,...,2097152,4194304.Astypicalstate-of-the-arttextembeddingmodelsaretrainedintwophases,alargeweakly-supervisedpre-trainingphaseandashortsupervisedphase,werunallexperimentsforbothphases.Forthelargesetting,weusethebestsettingsfoundviasmallexperiments.Wetrainasinglemodelonsequencesoflength512with512contextualdocuments,evaluatingonthefullMTEBbenchmark(Muennighoffetal.,2022).Thisincludestasksfromretrievalaswellastaskslikeclassication,clustering,andreranking.TrainingDataandMetricsWetrainonthemeta-datasetscollectedinNussbaumetal.(2024)fortrainingtextembeddingmodels.Thiscollectionofdatasetsincludesdatafrom24datasetsscrapedfromwebsourcessuchasWikipediaandReddit.Ourunsupervisedtrainingphasetrainson200Mweakly-superviseddatapointsscrapedfromlargeinternetsourcessuchasRedditandWikipedia.Thesupervisedtrainingphaseincludes1.8Mhuman-writtenquery-documentpairsintendedfortextretrieval,andisaggregatedfrompopularretrievaldatasetssuchasHotpotQAandMSMARCO(Yangetal.,2018;Bajajetal.,2018).Forourfullmodel,wealsoconsidersupervisedtrainingontheBGEmeta-datasets(Xiaoetal.,2024).WeevaluateourmodelsusingNDCG@10,aconventionalretrievalmetricthatenablescomparisonacrossmanydisparatedatasets.ImplementationWhenpartitioningourdatasetintobatches,weencodedocumentsandqueriesusingGTR(Nietal.,2021)andimplementourclusteringalgorithmontopofFAISS(Douzeetal.,2024).Weclusterper-domainfor100stepsandtakethebestclusteringoutof3attempts.WeselectNomicBERTasourpre-trainedmodelbackbone(Nussbaumetal.,2024),whichhas137Mparameters.Weprependalltextswithshorttask-specicprexestoidentifyeachtask;prexesarelistedinSection10.7.Whenpooling,wepoolovertexttokensonly,nevercontextualtokens.TrainingWeinitializebothM1andM2usingtheBERT-basemodelfromNussbaumetal.(2024)thatincludesashattention.Weightsaresharedbetweenϕandψ,butnotablynotbetweenM1andM2.Forallexperiments,wetrainwiththeAdamoptimizerwith1000stepsofwarmuptoalearningrateof2·10−5andlinearlydecayto0throughouttraining.Forthelteringmodelweselectnomic-embed-v1whichwastrainedonthesamedatasets(Nussbaumetal.,2024).Wetrainforthreeepochsunlessotherwisespecied.Wesetthemaximumsequencelengthforallinputsto512andthenumberofcontextualinputsto512(sothesecond-stagemodelhasaninputlengthof1024).Whencomputingcontrastiveloss,weuseaxedtemperatureofτ=0.02.Whensequencedropoutisenabledinourcontextualarchitecture,wesetcontextualinputtokenstonullvectorswithauniformprobabilityp=0.005.Ifthebatchsizeexceedsthenumberofcontextualdocuments,werandomlysampletoproducecontextualinputs.7

59.9✓

0.3790.7

NDCG@10

16384-

512512

0.8177.7

TrainlossTrainacc.

0.6880.9

61.7✓

62.4✓✓

0.3990.3

512512

nomic-embed-v174.143.985.255.752.882.130.162.39stella-base-en-v275.344.986.558.850.183.032.562.61bge-base-en-v1.575.545.886.658.953.382.431.163.56GIST-Embedding-v076.046.286.359.452.383.530.963.71gte-base-en-v1.577.246.885.357.754.182.031.264.11

ClssfctnClusterPairClsRerankRetrvlSTSSumm.Mean

Table2:Performanceofmodelswith250MorfewerparametersontheMTEBbenchmarkfortextembeddingmodels.“Random”indicatestheperformanceofourmodelwithrandomtrainingdocumentsincludedinsteadofper-domaincontextualdocuments.6RESULTSThemainresultsarehighlightedinTable1andSection6.Inthesmallersetting,weobservethatbothadversarialcontrastivelearningandourcontextualarchitectureimproveperformancecomparedtovanillabiencodertraining.Weobservethelargestimprovementwhenwecombinethesetechniques.ContextualbatchingAftercontrollingforbatchsizeandlteringforfalsenegatives,weobserveastrongcorrelation(visualizedinFigure2)betweenbatchdifcultyanddownstreamperformance:reorderingdatapointstomakebatchesharderdenitivelyenhancesoveralllearning.Thiscorrob-oratespriorndings(Xiongetal.,2020;Quetal.,2021)andtheory(Zhang&Stratos,2021)thatmoredifcultbatchesincontrastivelearningformabetteroverallgradientapproximationandlearnmoreeffectively.Section6showcasesmodelperformanceacrossbatchandclustersizesafterbothphasesoftraining.Weobservethatalthoughalargebatchandclustersizeareusefulwhenlteringisnotenacted,whenincludingltering,smallercluster(andharder)areclearlybetter,andlargebatchesdonotaddmuch.Whencomparinglteredtonon-lteredmodels(Figure4),lteringfalsenegativesclearlyimprovesperformance.ContextualarchitectureInadditiontoadversarialbatching,wecompareourcontextualarchitec-turetoabiencoderacrossthedatasetsofBEIRinTable1(fullresultsinappendix).Ourarchitecturegenerallymatchesorimprovesperformanceonalldownstreamdatasets,withlargestimprovementsinArguAnaandSciFact,twoofthesmallerandmoreout-of-domaindatasets.Full-scaletrainingFigure5showsourmodels’performancewhentrainedformultipleepochsonthesuperviseddatasets,relativetothebestsimilar-sizedembeddingmodel(dashedline).WendbestperformancewhentrainingforfourepochsontheBGEmeta-datasets.Althoughourbestmodeldoesuseasinglehardnegativeperquery,wearestillabletotoachievestate-of-the-artperformancewithoutusinganyhardnegatives.Forournalmodel(cde-small-v1),weselectthebestofthesupervisedmodels,whichcomesfromnetuningontheBGEdataset.OnMTEB,cde-small-v1obtainsstate-of-the-artresultscomparedtomodelsofthesamesize.Althoughinspiredbyproblemsinthespecicdomainoftextretrieval,weobservethatourapproachimprovesembeddingperformanceinalldomains,includingclustering,classication,andsemanticsimilarity.Wealsoevaluatea“randomdocuments”baseline,wherewesamplerandomdocumentsfromthetrainingdatasettosimulateascenariowherewelackaccesstothetestcorpus.Inthissetting,wedroparound1.2pointsonaverageacrossalltasks;theSTStasksinparticularappeartoproducerepresentationsthatareclosetocontext-agnostic.7ANALYSISHowhardareourclusters?Toanalysistherelationshipbetweenclustersizeinourclusteringalgorithmandtheoverallaveragedifcultyofin-batchnegatives,wemeasuretheaveragedifculty8

cde-small-v1[Random]81.346.684.155.351.181.431.663.81[Contextual]81.748.384.756.753.381.631.265.00

Figure3:Biencoderperformancewithltering(left)andwithout(right)acrossbatchandclustersizesduringunsupervisedcontrastivepre-training.Withltering,smallclustersizesclearlyimproveperformance,andlargerbatchsizesdonot.of1000batchesacrossavarietyofbatchandclustersizesandplotthedatainFigure6.Weobservethatlargerbatchesbringeasiernon-negativeexamples,anddecreasingclustersizeclearlyincreasestheaveragehardnessofnegativeexamplesinagivencluster.9

Figure2:Performancevs.averagebatchdifculty(asmeasuredbylossattheendofpre-trainingandsupervisedtraining)acrossbatchsizes,aftersupervisedcontrastivetraining.Withinagivenbatchsize,weobserveaclearincreaseinperformancebymakingindividualbatchesharder.CorrelationsarePearson.

Figure4:Impactoflteringduringtrainingacrossvariousbatchandclustersizes.Eachdotisabiencoderpretrainedwithadifferentbatchandclustersize.

Figure7:ImpactofcontextbytestingourmodelwithdifferentStackexchangeforuminputtypes.Y-axisindicatestheinputdo-main,X-axisindicatesthetestdomain.DarksquarescomewithinonepointNDCG@10.Whichcontextualdocumentshelp?ToconrmthattheCDEmodelisutilizingcontextualinformationfromDweconsiderhowdifferentcontextualdocumentshelpforagivendocuentd.Figure7measuresresultsonCQADupstack,acollectionofStackExchangeforumposts.WerandomlysampleinputstofromDfromadomain(x-axis)andusethemasinputtothedownstreamtaskdmarkedalongthey-axis.Wemarkasquareasredifitsscorecomeswithin1pointofNDCGofthetopscoreforitsdomain.Generallyutilizingin-domainworksbest,buttherearesomecrossoverinteractions.8CONCLUSIONWeproposetwoimprovementstotraditionalbiencodermodelsforgeneratingembeddings.Therstimprovementinvolvesanalgorithmforreorderingtrainingdatapointstomakebatchesharderandimprovesvanillatrainingwithminimalchanges.Oursecondimprovementinvolvesanewcorpus-awarearchitectureforretrievalandallowsustotrainastate-of-the-arttextembeddingmodel.10

Figure6:Averagedifcultyofin-batchnegativesasmeasuredbyasurrogatemodelasclustersizeandbatchsizechange.

Figure5:PerformanceonMTEBacrossepochsofsupervisedtrainingontheNomicandBGEsupervisedmeta-datasets.

9ACKNOWLEDGEMENTSThankstoOrionWeller,VinSachidananda,andZachNussbaumforvaluablefeedbackonthisresearch.WewouldalsoliketoacknowledgetoNomicandHyperbolicforprovidingthecomputenecessarytoconductthisresearch.ThisworkwaspartiallysupportedbyIntelligenceAdvancedResearchProjectsActivity(IARPA),viatheHIATUSProgram#2022-22072200003.JMissupportedbyanNSFGFRPfellowship.11

REFERENCESPayalBajaj,DanielCampos,NickCraswell,LiDeng,JianfengGao,XiaodongLiu,RanganMa-jumder,AndrewMcNamara,BhaskarMitra,TriNguyen,MirRosenberg,XiaSong,AlinaStoica,SaurabhTiwary,andTongWang.Msmarco:Ahumangeneratedmachinereadingcomprehensiondataset,2018.URLhttps://arxiv.org/abs/1611.09268.SebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMil-lican,GeorgevandenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,DiegodeLasCasas,AureliaGuy,JacobMenick,RomanRing,TomHennigan,SaffronHuang,LorenMaggiore,ChrisJones,AlbinCassirer,AndyBrock,MichelaPaganini,GeoffreyIrving,OriolVinyals,SimonOsindero,KarenSimonyan,JackW.Rae,ErichElsen,andLaurentSifre.Improvinglanguagemodelsbyretrievingfromtrillionsoftokens,2022.TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton.Asimpleframeworkforcontrastivelearningofvisualrepresentations,2020.WilliamCosterandDavidKauchak.SimpleEnglishWikipedia:Anewtextsimplicationtask.InDekangLin,YujiMatsumoto,andRadaMihalcea(eds.),Proceedingsofthe49thAnnualMeetingoftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pp.665–669,Portland,Oregon,USA,June2011.AssociationforComputationalLinguistics.URLhttps://aclanthology.org/P11-2117.ZhuyunDai,VincentY.Zhao,JiMa,YiLuan,JianmoNi,JingLu,AntonBakalov,KelvinGuu,KeithB.Hall,andMing-WeiChang.Promptagator:Few-shotdenseretrievalfrom8examples,2022.MatthijsDouze,AlexandrGuzhva,ChengqiDeng,JeffJohnson,GergelySzilvasy,Pierre-EmmanuelMazar´e,MariaLomeli,LucasHosseini,andHerv´eJ´egou.Thefaisslibrary,2024.AnthonyFader,LukeZettlemoyer,andOrenEtzioni.OpenQuestionAnsweringOverCuratedandExtractedKnowledgeBases.InKDD,2014.AngelaFan,YacineJernite,EthanPerez,DavidGrangier,JasonWeston,andMichaelAuli.ELI5:longformquestionanswering.InAnnaKorhonen,DavidR.Traum,andLlu´ısM`arquez(eds.),Proceedingsofthe57thConferenceoftheAssociationforComputationalLinguistics,ACL2019,Florence,Italy,July28-August2,2019,Volume1:LongPapers,pp.3558–3567.AssociationforComputationalLinguistics,2019.doi:10.18653/v1/p19-1346.URLhttps://doi.org/10.18653/v1/p19-1346.KatjaFilippovaandYaseminAltun.Overcomingthelackofparalleldatainsentencecompression.InDavidYarowsky,TimothyBaldwin,AnnaKorhonen,KarenLivescu,andStevenBethard(eds.),Proceedingsofthe2013ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.1481–1491,Seattle,Washington,USA,October2013.AssociationforComputationalLinguistics.URLhttps://aclanthology.org/D13-1155.WikimediaFoundation.Wikimediadownloads,2024.URLhttps://dumps.wikimedia.org.LuyuGaoandJamieCallan.Unsupervisedcorpusawarelanguagemodelpre-trainingfordensepassageretrieval,2021.LuyuGao,YunyiZhang,JiaweiHan,andJamieCallan.Scalingdeepcontrastivelearningbatchsizeundermemorylimitedsetup,2021.MartaGarnelo,DanRosenbaum,ChrisJ.Maddison,TiagoRamalho,DavidSaxton,MurrayShana-han,YeeWhyeTeh,DaniloJ.Rezende,andS.M.AliEslami.Conditionalneuralprocesses,2018.MansiGupta,NitishKulkarni,RaghuveerChanda,AnirudhaRayasam,andZacharyCLipton.Amazonqa:Areview-basedquestionansweringtask,2019.SuchinGururangan,MargaretLi,MikeLewis,WeijiaShi,TimAlthoff,NoahA.Smith,andLukeZettlemoyer.Scalingexpertlanguagemodelswithunsuperviseddomaindiscovery,2023.12

FelixHamborg,NormanMeuschke,CorinnaBreitinger,andBelaGipp.news-please:Agenericnewscrawlerandextractor.InProceedingsofthe15thInternationalSymposiumofInformationScience,pp.218–223,March2017.doi:10.5281/zenodo.4120316.ChristopherHideyandKathyMcKeown.IdentifyingcausalrelationsusingparallelWikipediaarticles.InKatrinErkandNoahA.Smith(eds.),Proceedingsofthe54thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pp.1424–1433,Berlin,Germany,August2016.AssociationforComputationalLinguistics.doi:10.18653/v1/P16-1135.URLhttps://aclanthology.org/P16-1135.SebastianHofst¨atter,Sheng-ChiehLin,Jheng-HongYang,JimmyLin,andAllanHanbury.Efcientlyteachinganeffectivedenseretrieverwithbalancedtopicawaresampling,2021.URLhttps://arxiv.org/abs/2104.06967.HamelHusain,Ho-HsiangWu,TiferetGazit,MiltiadisAllamanis,andMarcBrockschmidt.CodeSearchNetchallenge:Evaluatingthestateofsemanticcodesearch.arXivpreprintarXiv:1909.09436,2019.GautierIzacard,MathildeCaron,LucasHosseini,SebastianRiedel,PiotrBojanowski,ArmandJoulin,andEdouardGrave.Unsuperviseddenseinformationretrievalwithcontrastivelearning,2022.MingukJang,Sae-YoungChung,andHyeWonChung.Test-timeadaptationviaself-trainingwithnearestneighborinformation,2023.VladimirKarpukhin,BarlasO˘guz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWentauYih.Densepassageretrievalforopen-domainquestionanswering,2020.UrvashiKhandelwal,OmerLevy,DanJurafsky,LukeZettlemoyer,andMikeLewis.Generalizationthroughmemorization:Nearestneighborlanguagemodels,2020.DanielKhashabi,AmosNg,TusharKhot,AshishSabharwal,HannanehHajishirzi,andChrisCallison-Burch.Gooaq:Openquestionansweringwithdiverseanswertypes,2021.OmarKhattabandMateiZaharia.Colbert:Efcientandeffectivepassagesearchviacontextualizedlateinteractionoverbert,2020.URLhttps://arxiv.org/abs/2004.12832.HyunjikKim,AndriyMnih,JonathanSchwarz,MartaGarnelo,AliEslami,DanRosenbaum,OriolVinyals,andYeeWhyeTeh.Attentiveneuralprocesses,2019.JannikKossen,NeilBand,ClareLyle,AidanN.Gomez,TomRainforth,andYarinGal.Self-attentionbetweendatapoints:Goingbeyondindividualinput-outputpairsindeeplearning,2022.MahnazKoupaeeandWilliamYangWang.Wikihow:Alargescaletextsummarizationdataset,2018.PatrickLewis,YuxiangWu,LinqingLiu,PasqualeMinervini,HeinrichK¨uttler,AleksandraPiktus,PontusStenetorp,andSebastianRiedel.Paq:65millionprobably-askedquestionsandwhatyoucandowiththem,2021.CanjiaLi,YingfeiSun,BenHe,LeWang,KaiHui,AndrewYates,LeSun,andJungangXu.NPRF:Aneuralpseudorelevancefeedbackframeworkforad-hocinformationretrieval.InEllenRiloff,DavidChiang,JuliaHockenmaier,andJun’ichiTsujii(eds.),Proceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.4482–4491,Brussels,Belgium,October-November2018.AssociationforComputationalLinguistics.doi:10.18653/v1/D18-1478.URLhttps://aclanthology.org/D18-1478.ZehanLi,XinZhang,YanzhaoZhang,DingkunLong,PengjunXie,andMeishanZhang.Towardsgeneraltextembeddingswithmulti-stagecontrastivelearning,2023.KyleLo,LucyLuWang,MarkNeumann,RodneyKinney,andDanS.Weld.S2orc:Thesemanticscholaropenresearchcorpus,2020.JiaweiMa,Po-YaoHuang,SainingXie,Shang-WenLi,LukeZettlemoyer,Shih-FuChang,Wen-TauYih,andHuXu.Mode:Clipdataexpertsviaclustering,2024.13

JohnX.Morris,VolodymyrKuleshov,VitalyShmatikov,andAlexanderM.Rush.Textembeddingsreveal(almost)asmuchastext,2023.NiklasMuennighoff,NouamaneTazi,Lo¨ıcMagne,andNilsReimers.Mteb:Massivetextembeddingbenchmark.arXivpreprintarXiv:2210.07316,2022.doi:10.48550/ARXIV.2210.07316.URLhttps://arxiv.org/abs/2210.07316.TungNguyenandAdityaGrover.Transformerneuralprocesses:Uncertainty-awaremetalearningviasequencemodeling,2023.JianmoNi,JiachengLi,andJulianMcAuley.Justifyingrecommendationsusingdistantly-labeledreviewsandne-grainedaspects.InKentaroInui,JingJiang,VincentNg,andXiaojunWan(eds.),Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.188–197,HongKong,China,November2019.AssociationforComputationalLinguistics.doi:10.18653/v1/D19-1018.URLhttps://aclanthology.org/D19-1018.JianmoNi,ChenQu,JingLu,ZhuyunDai,GustavoHern´andez´Abrego,JiMa,VincentY.Zhao,YiLuan,KeithB.Hall,Ming-WeiChang,andYinfeiYang.Largedualencodersaregeneralizableretrievers,2021.RodrigoNogueiraandKyunghyunCho.Passagere-rankingwithbert,2020.ZachNussbaum,JohnX.Morris,BrandonDuderstadt,andAndriyMulyar.Nomicembed:Trainingareproduciblelongcontexttextembedder,2024.YingqiQu,YuchenDing,JingLiu,KaiLiu,RuiyangRen,WayneXinZhao,DaxiangDong,HuaWu,andHaifengWang.Rocketqa:Anoptimizedtrainingapproachtodensepassageretrievalforopen-domainquestionanswering,2021.PranavRajpurkar,JianZhang,KonstantinLopyrev,andPercyLiang.SQuAD:100,000+QuestionsforMachineComprehensionofText.arXive-prints,art.arXiv:1606.05250,2016.NilsReimers,ElliotChoi,AmrKayid,AlekhyaNandula,ManojGovindassamy,andAb-dullahElkady.Introducingembedv3,Nov2023.URLhttps://txt.cohere.com/introducing-embed-v3/.StephenRobertsonandHugoZaragoza.TheProbabilisticRelevanceFramework:BM25andBeyond.NowPublishersInc.,2009.JoshuaRobinson,Ching-YaoChuang,SuvritSra,andStefanieJegelka.Contrastivelearningwithhardnegativesamples,2021.J.J.Rocchio.Relevancefeedbackininformationretrieval.1971.URLhttps://api.semanticscholar.org/CorpusID:61859400.VinSachidananda,ZiyiYang,andChenguangZhu.Globalselectionofcontrastivebatchesviaopti-mizationonsamplepermutations.InAndreasKrause,EmmaBrunskill,KyunghyunCho,BarbaraEngelhardt,SivanSabato,andJonathanScarlett(eds.),Proceedingsofthe40thInternationalConferenceonMachineLearning,volume202ofProceedingsofMachineLearningResearch,pp.29542–29562.PMLR,23–29Jul2023.URLhttps://proceedings.mlr.press/v202/sachidananda23a.html.KeshavSanthanam,OmarKhattab,JonSaad-Falcon,ChristopherPotts,andMateiZaharia.Colbertv2:Effectiveandefcientretrievalvialightweightlateinteraction,2022.URLhttps://arxiv.org/abs/2112.01488.ChristopherSciavolino.Towardsuniversaldenseretrievalforopen-domainquestionanswering,2021.AbigailSee,PeterJ.Liu,andChristopherD.Manning.Gettothepoint:Summarizationwithpointer-generatornetworks.InProceedingsofthe55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pp.1073–1083,Vancouver,Canada,July2017.AssociationforComputationalLinguistics.doi:10.18653/v1/P17-1099.URLhttps://www.aclweb.org/anthology/P17-1099.14

WeijiaShi,SewonMin,MariaLomeli,ChuntingZhou,MargaretLi,GergelySzilvasy,RichJames,XiVictoriaLin,NoahA.Smith,LukeZettlemoyer,ScottYih,andMikeLewis.In-contextpretraining:Languagemodelingbeyonddocumentboundaries,2024.HongjinSu,WeijiaShi,JungoKasai,YizhongWang,YushiHu,MariOstendorf,WentauYih,NoahA.Smith,LukeZettlemoyer,andTaoYu.Oneembedder,anytask:Instruction-netunedtextembeddings,2023.MujeenSung,JungsooPark,JaewooKang,DanqiChen,andJinhyukLee.Optimizingtest-timequeryrepresentationsfordenseretrieval,2023.NandanThakur,NilsReimers,AndreasR¨uckl´e,AbhishekSrivastava,andIrynaGurevych.Beir:Aheterogenousbenchmarkforzero-shotevaluationofinformationretrievalmodels,2021.LiangWang,NanYang,XiaolongHuang,BinxingJiao,LinjunYang,DaxinJiang,RanganMajumder,andFuruWei.Simlm:Pre-trainingwithrepresentationbottleneckfordensepassageretrieval,2023.LiangWang,NanYang,XiaolongHuang,BinxingJiao,LinjunYang,DaxinJiang,RanganMajumder,andFuruWei.Textembeddingsbyweakly-supervisedcontrastivepre-training,2024.XiaoWang,CraigMacdonald,NicolaTonellotto,andIadhOunis.Pseudo-relevancefeedbackformultiplerepresentationdenseretrieval.InProceedingsofthe2021ACMSIGIRInternationalConferenceonTheoryofInformationRetrieval,ICTIR’21.ACM,July2021.doi:10.1145/3471158.3472250.URLhttp://dx.doi.org/10.1145/3471158.3472250.ShitaoXiao,ZhengLiu,PeitianZhang,NiklasMuennighoff,DefuLian,andJian-YunNie.C-pack:Packagedresourcestoadvancegeneralchineseembedding,2024.URLhttps://arxiv.org/abs/2309.07597.LeeXiong,ChenyanXiong,YeLi,Kwok-FungTang,JialinLiu,PaulBennett,JunaidAhmed,andArnoldOverwijk.Approximatenearestneighbornegativecontrastivelearningfordensetextretrieval,2020.URLhttps://arxiv.org/abs/2007.00808.CanwenXu,DayaGuo,NanDuan,andJulianMcAuley.Laprador:Unsupervisedpretraineddenseretrieverforzero-shottextretrieval,2022.ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,WilliamW.Cohen,RuslanSalakhutdinov,andChristopherD.Manning.Hotpotqa:Adatasetfordiverse,explainablemulti-hopquestionanswering,2018.URLhttps://arxiv.org/abs/1809.09600.WenzhengZhangandKarlStratos.Understandinghardnegativesinnoisecontrastiveestimation,2021.15

10SUPPLEMENTARYMATERIAL10.1COMPUTATIONALRESOURCEUSAGEWepre-trainallmodelson8NVIDIAH100GPUs.Intheslowestsetting,trainingabiencoderforasingleunsupervisedepoch(235Mpairs)takesapproximatelyoneday.Trainingourcontextualarchiectureforasingleepochtakesapproximatelytwodays.Shortersequence-lengthexperimentsare10-20xfaster,andcanberunonasingleGPU.10.2INITIALEXPERIMENTSWeconductedtwopreliminaryexperimentstoverify(i)theneedforcontextualtrainingstrategyand(ii)theneedforin-batchfalsenegativelteringwhendoingadversarialcontrastivelearningonarealdataset.Preliminaryexperiment(i).Weconductapreliminaryexperimenttoverifythisissue.StartingfromseveraltrainedretrievalsystemswecomputeperformanceonavarietyofdifferenttasksfromtheBEIRdataset.AdditionallywecomputetheIDFstatisticsfromthedatasets,andcomparethedivergencefromthebaseIDFstatisticsofthetrainingset.Figure8showsthatdatasetswithhigh-divergencehaveveryhighcorrelationwiththeaccuracydegradationofmodelswhenmeasuredincomparisontoBM25,whichisabletomeasureandadapttostatisticsofthetestcorpus.

Figure8:Analysisofdomainshiftforpopularneuralretrievalmethods.PerformancedifferencefromBM25(y-axis)correlateswiththedifferentinIDFofthetestcorpusDformthetrainingcorpusDT.Preliminaryexperiment(ii).Weselectarandomdocumentfromanunsupervisedcorpusandlookatitsnearestneighbors,displayedinTable3.Weobservethatthenearestneighborstoagivendocumentinalargecorpusareveryclose;infact,manyofthemcouldbeconsideredvaliddocumentsforthegivenqueryaswell.Thischallengemotivatesourembeddingcontextualization.Inthissection,wedescribetwocom-plementarymethodsforremediation,(a)acontextualtrainingmethod,(b)acontextualencodingmethod.10.3INTERACTIONSBETWEENCONTRASTIVELOSSANDDISTRIBUTEDDATAPARALLELTheauthorsnotethatitcanbenotoriouslydifculttotrainmodelsusingbothcontrastivelossandthedistributeddataparallel(DDP)setting.Inparticular,whenaggregatingsamplesbetweenGPUs,ifanyartifactrevealswhichGPUamodelcamefrom(forexample,iftheGPUmodelweightsareinitializedslightlydifferently)thanthemodelcanquicklydeterioratetoasuboptimalsolution,each16

QueryDocument

GPUlearningadifferentnalmodeland“cheating”toclassifysamplesbasedonwhichGPUtheycamefrom.Thisissueismadeextradifcultbythefactthatgradient-syncingmustbedisabledforlarge-batchcontrastivelearningtoworkefciently.Ifgradientsyncingbecomestotallydisabled,thetrainingsilentlydivergesaseachmodellearnsadegeneratesolution.Weadvisepractitionerstotakecarewhencontrollinggradient-syncingandrunmanycontrolexperimentstodetermineperformanceequivalencebetweenDDPandnon-DDPscenarios.Onepotentialbenetofourmethodisthatitgreatlydecreasesthenumberofhardnegativesrequiredperbatch,whichmeansthatnegative-sharingacrossGPUsmaynotbenecessaryinmostsettings.Ifpossible,themostsanity-preservingwaytoperformcontrastivetrainingcouldbeto10.4REMOVINGPOSITIONALITYWITHROTARYEMBEDDINGSOnedetailofourmodelarchitectureisthatitdoesnottrackpositionalitybetweendatasetinputtokens.AlthoughdisablingpositionalitywouldbetrivialanaBERT-likeencodermodelthatuseslearnedpositionalembeddings,weuseaversionofBERTwithrotarypositionalembeddingswhichinjectpositionalinformationateachlayerofthetransformer.Tocircumventthisstep,wemodifythemodelinternalstosetdatasetinputtokenstozerofortheself-attentionsteponly,andaddaresidualconnectionpropagatingthedatasetinputtokenspasttheattentionphase.10.5ADDITIONALRESULTSSection10.5showsweepsoverbatchandclustersizesunderoursmallexperimentalsettingswhenperformingunsupervisedpretrainingwithcontextualarchitecture.Weseesimilartrendstothoseobservedwiththebiencoderarchitecture,howeverwenotethatperformanceishigheracrosstheboardandourtransductivemodelisabletoperformwellevenathigherclustersizesandlowbatchsizes.Oneconfoundingfactorintheseexperimentsisthatsincethenumberofcontextualdocumentsisxed,thenumberofdifferentcontextualinputsseenduringtrainingdecreaseswithhigherbatchsize.17

C1feeinmystatement?whyisthereanextrachargeonmystatement?whatisthisfeeforcardpayment?whywasafeechargedformycardpayment?whydoihaveduplicatetransactionsforonepur-chase?whywasmytransactionchargedtwice?ihavetwoofthesamechargesonmyaccount!whywasmytransactionchargedtwice?mytransactionwentthroughbutiwaschargedafee.why?whywasafeechargedformytransfer?myaccountshowsihavebeenchargedtwiceforthesamemeal.[...]willigetextracharges?whywasafeechargedformytransfer?igotchargedindoubleandwantarefundwhywasmytransactionchargedtwice?wheredoipaywithmydebitorcreditcard?whyismycardnotaccepted?whydidigetchargedafeeformycardpayment?whywasafeechargedformycardpayment?mystatementshowsdifferenttransactiontimes.whywasmytransactionchargedtwice?

whyisthereanextra

lookslikemycardpaymentwasduplicatedafterall.[...]

Table3:Nearest-neighborstoasinglequeryinalargeunsuperviseddataset.

Figure10:Correlationbetweenbatchdifcultyandperforamnceaftersupervisedtraining.Thismightexplainpartofwhyperformancestagnateswithhigherbatchsizes;increasingthebatchsizedecreasesthetotalnumberoflearningexamplesseenbyourcontextualmodel.Supervisedtraining:difcultycorrelations.InSection10.5weplotthecorrelationbetweenbatchdifcultyanddownstreamperformanceacrossclustersizes(andwithinbatchsizes)inthesupervisedsetting.Inthiscasewealsoseethebestperformancethroughthemostdifcultclusters.18

Figure9:Contextualperformancewithltering(left)andwithout(right)acrossbatchandclustersizesduringunsupervisedcontrastivepre-training.Here,clusteringwithsmallclustersizesclearlyimprovesperformance,andlargerbatchsizesdonot.

Figure12:Modelperformancevs.clustersizewithandwithoutltering.Whenfalsenegativelteringisenabled,weseemoreimprovementsinperformancefromclusteringatsmallclustersizes.

Figure13:Modelperformancevs.batchsizewithandwithoutltering.Withandwithoutltering,theoptimalbatchsizerangesbetween102and104;performancestartstodecreaseasbatchsizegrowstoolarge.Supervisedtraining:fullresults.WeplotthefullresultsofallsupervisedtrainingexperimentsinSection10.5.Ourexperimentsinthissetting(usingtheminednegativesfromtheNomicsupervisedmeta-datasets)generallyshowdecreasingperformancewithadditionalhardnegatives.TSPPacking.Wecomparerandomlypackingclustersintobatchesvs.agreedytravelingsalesman-stylesolution,similarto(Shietal.,2024).Inourscenario,werstclusterdatapoints,thenndthecentroidembeddingofeachcluster.Webeginpackingbyrandomlyselectingacluster,andthenchoosethenextclusterbyndingtheclusterwiththeclosestcentroidtothecurrentone.ResultsareshowninFigure14.Althoughtheseresultsappearslightlynoisy,weseeanimprovementfromTSP-stylepackingespeciallyatsmallerclustersizes(wherepackinghasanoutsizedimpact).Wethereforeopttousethispackingprocedureforourmainmodel.ImpactofcontextsizeWeconsidercontextualembeddingsmightmoveinspaceastheircondition-ingvaries.Section10.5displaysafewqualitativeexamples.WegenerateembeddingsforrandomlysampleddocumentsfromtheTREC-CoviddatasetandvisualizetheirembeddingswithPCA,whereuniquedocumentinputswithdifferentcontextualembeddingsarevisualizedinthesamecolor.Bychangingonlytheconditioningwereshapetheembeddingspaceandourmodelproducesdifferentembeddingforthesametext.Notethatalthoughtheembeddingsareclearlymovinginresponsetochangingthecontextualinputs,theystillremainclosertoeachotherthantodifferentdocuments.19

Figure11:Performanceofallsupervisedmodels,acrossnumbersofhardnegatives.

Figure16:PerformanceofCDEmodelasthenumberofcontextualexamplesincreases.Wealsoconsiderhowadditionalcontextisimprovingourmodel.Becausethemodelincludesanoptionalnulltoken,wecansupplyanynumberofcontextualinputs.Weplotourmodel’sperformanceacrosscontextsizesinFigure10.5.Weseethatourmodelisabletoutilizepartialcontextwindowsizes,andevenperformreasonablywithnocontext(i.e.allnulltokeninputs)butoffersthebestperformancegivenafullcontextwindowsize.10.6CLUSTERTEXTEXAMPLESWeincluderandomexamplesfromaclustergatheredfromoursuperviseddataset,showninTable4.ThisparticularclusterappearstobeacombinationofdocumentsaboutcountypopulationsintheUntiedStates(inKentucky,Iowa,Pennsylvania,etc.)anddocumentsaboutcriminaltrials(mentioninghearings,depositions,andcourts).10.7TASKPREFIXESPrexesarehand-writtenforeachdatasetinbothmeta-trainingsets.WefollowthesameprexselectionprocedureasNussbaumetal.(2024),inspiredbyReimersetal.(2023):•search

document•classification20

Figure14:Pre-trainingwithTSPvs.randombatchingacrossclustersizes.

Figure15:Eachcolorindicatesasingledocu-mentinputd.Differentpointsrepresentdif-ferentvaluesϕ(d;D)fordifferentcontexts.

query•search

document

howlongcanittakeforatrial

clintoncountyisacountylocatedintheu.s.stateofkentucky.asofthe2010census,thepopulationwas10,272.itscountyseatisalbany.thecountywasformedin1835andnamedfordewittclinton,theseventhgovernorofnewyork.itisaprohibitionordrycounty.

whatisthepopulationinidaho

thecityofwhitinghadapopulationof177asofjuly1,2017.whitingranksinthelowerquartileforpopu-lationdensityanddiversityindexwhencomparedtotheothercities,towns[...]

ithasalsoinitiatedseveralprogramsdesignedtoimprovetheeffectivenessofthecourtsystem.aprimaryfunctionofthesupremecourtistoensure[...]

populationofioscocountymichigan

thepreliminaryhearingphaseofthetrialusuallytakesplace5-6daysafteranarraignment.inthecaseofamisdemeanor[...]

atheadcountypopulation

lewiston,idpopulationandraces.asof2010-2014,thetotalpopulationoflewistonis32,178,whichis4.12%morethanitwasin2000.[...]

ndanswers.sentencing.afteracriminaldefendantisconvictedorpleadsguilty,ajudgewilldecide[...]

populationofbreckenridgemi

manson,iowa.mansonisacityincalhouncounty,iowa,unitedstates.thepopulationwas1,690atthe2010census.

whatisthepopulationofmanson,ia

whathappensifyoudon’tshowupforjury

cleareldisaboroughandthecountyseatofcleareldcounty,pennsylvania,unitedstates.thepopulationwas6,215atthe2010census,andtheboroughispartofthedubois,pamicropolitanstatis-ticalarea,aswellasthelargerstatecollege-dubois,pacombinedstatisticalarea.

whathappensafterasentencinghearing

depositionsarecommonlyusedincivillitigation(suitsformoneydamagesorequitablerelief)[...]

populationofcleareldcountypa

whatcasesrequirestrictscrutiny

Table4:Sixteensamplesfromaclusterouralgorithmndsinthesupervisedtrainingdata.Thefullclustersizeis256pointsoutofadatasetof1.5M.21

query

populationclintonky

whiting,kspopulation

thestrictscrutinystandardisoneofthreeemployedbythecourtsinreviewinglawsandgovernmentpoli-cies.therationalbasis[...]

breckenridge,michigan.breckenridgeisavillageingratiotcountyintheu.s.stateofmichigan.thepopulationwas1,328atthe2010census.thevillageislocatedinwheelertownship.

canadepositionbeusedinacriminalcase

functionofstatesupremecourts

with25,420people,ioscocountyisthe55thmostpopulatedcountyinthestateofmichiganoutof83counties.butwatchout,ioscocounty,becausegladwincountywith25,411peopleandmanisteecountywith24,420peoplearerightbehindyou.

atheadcounty,montana.atheadcountyisacountylocatedintheu.s.stateofmontana.asofthe2010census,thepopulationwas90,928,makingit[...]

whatisthepopulationoflewistonid

whathappensifyoudon’tshowupforjurydutyincalifornia?a:accordingtocaliforniacourts,judicialbranchofcalifornia,ifacitizenfailstoshowupforjuryduty,thejurorcanaccruenesupto$1,500.ifservicepresentsanunduehardship,ajurorcanrequestapostponementortobeexcused.otherwise,citizensarenotexemptfromjuryduty.

idaho’spopulationgrowstonearly1.7million.idaho’spopulationgrewby1.2percentbetweenmid-2014andmid-2015,the12thstrongestin-creaseamongthestatesandfour-tenthsofaper-centagepointaheadofthenationalgrowthrate.

Reddita64,978,9440.28PAQLewisetal.(2021)52,953,0880.23AmazonReviewsNietal.(2019)38,682,6240.16S2ORCTitleAbstractLoetal.(2020)354385920.15WikiAnswersFaderetal.(2014)9,912,3200.04S2ORCCitationTitlesLoetal.(2020)7,585,7920.03S2ORCAbstractCitationLoetal.(2020)7,503,8720.03S2ORCAbstractBodyLoetal.(2020)6,389,7600.03WikipediaTitleBodyFoundation(2024)6,078,4640.03GooaqKhashabietal.(2021)1,245,1840.01CodesearchHusainetal.(2019)835,584<.01AGNews?409,600<.01CCNewsHamborgetal.(2017)344,064<.01NPRb344,064<.01CNNSeeetal.(2017)278,528<.01YahooTitle-Answerc262,144<.01AmazonQAGuptaetal.(2019)212,992<.01YahooTitle-Questiond196,608<.01SentenceCompressionFilippova&Altun(2013)163,840<.01YahooQAe131,072<.01ELI5Fanetal.(2019)98,304<.01AltlexHidey&McKeown(2016)98,304<.01WikihowKoupaee&Wang(2018)81,920<.01SimpleWikiCoster&Kauchak(2011)81,920<.01StackExchangeDuplicateQuestionsf65,536<.01StackExchangeTitleBodyg65,536<.01StackExchangeBodyBodyh65,536<.01QuoraDuplicateQuestionsi32,768<.01SQuADRajpurkaretal.(2016)16,384<.01

Table5:DistributionofpretrainingdatasetscuratedinNussbaumetal.(2024).

DatasetDatapoints%Dataset

ahttps://huggingface.co/datasets/sentence-transformers/reddit-title-bodybhttps://files.pushshift.io/news/chttps://www.kaggle.com/soumikrakshit/yahoo-answers-datasetdhttps://www.kaggle.com/soumikrakshit/yahoo-answers-datasetehttps://www.kaggle.com/soumikrakshit/yahoo-answers-datasetfhttps://data.stackexchange.com/apple/query/fork/1456963ghttps://data.stackexchange.com/apple/query/fork/1456963hhttps://data.stackexchange.com/apple/query/fork/1456963ihttps://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs•clustering10.8UNSUPERVISEDTRAININGDATASETSWetrainon234Mweaklysupervisedquery-documentpairscollectedfortrainingtextembeddingmodelsinNussbaumetal.(2024).Thefulldistributionof29datasetsisshowninTable5.Redditalonemakesupover25%ofthedatadistribution,with19ofthedatasetscomprisingunder1%ofthetotaldata.22

Total234,553,3441

NFCorpus3233,633SciFact3005,183ArguAna1,4068,674SciDocs1,00025,657TREC-COVID50171,332Quora5,000522,931NaturalQuestions3,4522,681,468MSMARCO6,9808,841,823

Table6:DistributionofBEIRevaluationdatasetsused,orderedbycorpussize.

Figure17:Systemperformance(trainingaccuracy)aswescalethesizeoftherst-stagemodelencoderonly.10.9BEIREVALUATIONDATASETSOurinitialexperimentsinvolveevaluatingonninedatasetsfromtheBEIRbenchmark.DatasetsaredetailedinTable6.Toenablefastevaluationatthisstage,weobtainthetop1024relevantdocumentstoeachdocumentwithGTR(Nietal.,2021)andrerankonlythesedocumentsatevaluationtime.10.10ADDITIONALMODELINGABLATIONSFirst-stagemodelsize.Oneconsiderationiswhetherwecanimproveoursystemwithoutaffectingsearchinferencetimebyscalingthenumberofparametersinthebackbonemodelonly.Westudythisaffectbyscalingthenumberoflayersinthetransformerbackboneoftherst-stagemodelfrom1tothefull12.ResultingperformanceisshowninSection10.10.Ourresultsshowthatscalingtherst-stagemodelhasasmallpositiveinuenceonmodelperformance.However,sincethetotalimprovementfroma12xincreaseinrst-stagemodelsizeislessthanonepercent,weconcludethatthesecond-stagemodelsizehasamuchlargerimpactonperformance.10.11HOWMANYTOKENSPERDOCUMENT?Weconsiderthequestionofhowmanytokensperdocumentisidealwhilekeepingthetotalnumberofdocumenttokensxed.ResultsperthenineevaluationdatasetsofBEIRareshowninSection10.11.10.12MTEBRETRIEVALEVALUATIONPERFORMANCEToevaluateonMTEB,wesubsamplecontextualdocumentsfromthefullcorpusavailableineachdatasetandmodality.Forretrieval,thiscorrespondstothecorpusitself(importantly,notthequeries);forothermodalities,wechoosethedefault“text”eldineachcasel.Forclassicationtasks,wesamplefromthetextside(nottheclassicationlabelsthemselves).23

DatasetQueriesDocuments

SupervisedBaseline49.340.538.345.085.038.473.643.135.059.487.718.370.579.928.252.8Contextual53.841.238.843.389.240.173.942.235.961.687.120.172.782.627.854.0

UnsupervisedBaseline54.841.424.740.274.439.963.835.035.748.688.220.272.062.219.248.0Contextual54.943.124.440.779.642.168.838.936.557.888.921.172.877.121.951.2

Table7:Results(NDCG@10)ontheretrievalsettingoftheMTEBbenchmark.Table7showsourmodelperformanceonalldatasetsintheMTEBretrievalcategory.WeseelargestimprovementsoverthebaselineontheArguAnaandTREC-Coviddatasets.24

MethodArgCQACFEVERDBPFEVERFiQAHPQAMSMRCNFCNQQUORASCIDSCIFTRECTOUCHEMean

Figure18:Performanceper-datasetaswescaletokens-per-document,whilekeepingthetotalnumberofcontextualtokensxed.Differentdomainspreferadifferentnumberoftokensperdocument.